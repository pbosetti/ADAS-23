---
title: "Regressione"
author: "Paolo Bosetti"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggExtra)
```

# Regressione di modelli lineari univariati

```{r}
set.seed(0)
N <- 100
df <- tibble(
  x = seq(-10, 10, length.out=N),
  y_nom = 2 * x + 0.1 * x^2,
  y = y_nom + rnorm(N, 0, 2)
)

df %>% 
  ggplot(aes(x=x, y=y)) + 
  geom_point() +
  geom_line(aes(y=y_nom), color="red")
```
Realizziamo il modello lineare:
$$
y_i = a + bx_i + c x_i^2+\varepsilon_i
$$

```{r}
(df.lm <- lm(y~x+I(x^2), data=df))
```
```{r}
summary(df.lm)
```
Riformuliamo il modello lineare togliendo il termine che ha un *p*-value eccessivo:

```{r}
df.lm <- lm(y~x + I(x^2) - 1, data=df)
summary(df.lm)
```
Grafici:

```{r}
df %>% 
  ggplot(aes(x=x, y=y)) +
  geom_smooth(
    method="lm",
    formula=df.lm$call$formula,
    level=0.99,
    lty=2
  ) +
  geom_point() +
  geom_line(aes(y=y_nom), color = "red")
```

Controllo i residui:

```{r}
p <- df %>% ggplot(aes(x=x, y=df.lm$residuals)) +
  geom_point()

p %>% ggMarginal(type="histogram", margins="y", bins=10)
shapiro.test(df.lm$residuals)

df %>% ggplot(aes(sample=df.lm$residuals)) + 
  geom_qq() + 
  geom_qq_line()
```

## Sotto-adattamento (*underfitting*)

Modello lineare di primo grado:

```{r}
df.lm2 <- lm(y~x, data=df)
summary(df.lm2)
```
```{r}
df$res2 <- df.lm2$residuals

df %>% ggplot(mapping=aes(sample=res2)) + 
  geom_qq() + 
  geom_qq_line()
shapiro.test(df.lm2$residuals)
```

```{r}
(df %>% ggplot(aes(x=x, y=res2)) +
  geom_point()) %>% 
  ggMarginal(type="histogram", margin="y", bins=10)
```

# Sovra-adattamento (*overfitting*)

```{r}
df <- df %>% mutate(
  subset = ifelse(x > -7.5 & x < 7.5, "in", "out")
)

df %>% 
  ggplot(aes(x=x, y=y, color=subset)) + 
  geom_smooth(
    data=filter(df, subset=="in"),
    method=lm,
    formula=y~poly(x, 10, raw=T),
    fullrange=TRUE
  ) +
  geom_point() +
  coord_cartesian(ylim=c(-30, 30))

```

## Predizione

La predizione $\hat y$ si ottiene con la funzione `predict()`:

```{r}
predict(df.lm2) %>% str()
```

Gli intervalli di confidenza attivando l'opzione `interval`:

```{r}
predict(df.lm2, interval="confidence")
```
Estrapolazione:

```{r}
df.new <- tibble(x=seq(-20, 20, 0.5))

df.new <- df.new %>% cbind(
  predict(
    df.lm, 
    newdata=df.new,
    interval="confidence",
    level=0.99
  )
)

df.new %>% 
  ggplot(aes(x=x, y=fit)) +
  geom_line() + 
  geom_ribbon(aes(ymin=lwr, ymax=upr), alpha=1/3) +
  geom_point(data=df, mapping=aes(x=x, y=y))

```
L'intervallo di confidenza **sui parametri** del modello si ottiene con `confint()`:

```{r}
confint(df.lm)
```

# Dati multivariati

```{r}
set.seed(10)
N <- 50

y <- function(x1, x2) 10 - 1*x1 + 0.1*x1^2 + 0.1*(-10*x2 + 1.5*x2^2) + 0.05 * x1 * x2

df <- expand.grid(
  x1=seq(0, 10, length.out=N),
  x2=seq(0, 10, length.out=N)
) %>% mutate(
  y_nom=y(x1, x2),
  y=y_nom + rnorm(N, 0, range(y_nom)/25) 
)

df %>% 
  ggplot(aes(x=x1, y=x2, z=y_nom)) + 
  geom_contour_filled()
```

```{r}
df.lm <- lm(y~poly(x1, 2, raw=T)*poly(x2, 2, raw=T), data=df)
summary(df.lm)
```

